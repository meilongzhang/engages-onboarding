# dl/problems/q1_relu.py
import torch

def relu(x):
    """
    Apply ReLU activation: ReLU(x) = max(0, x)

    Args:
        x: torch.Tensor

    Returns:
        torch.Tensor with ReLU applied
    """
    pass